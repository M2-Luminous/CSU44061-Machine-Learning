{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Network Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 13, 13, 32)        320       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 64)          18496     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                23050     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41866 (163.54 KB)\n",
      "Trainable params: 41866 (163.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2455 - accuracy: 0.9310 - val_loss: 0.0957 - val_accuracy: 0.9758\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0879 - accuracy: 0.9763 - val_loss: 0.0865 - val_accuracy: 0.9776\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0652 - accuracy: 0.9829 - val_loss: 0.0718 - val_accuracy: 0.9813\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0536 - accuracy: 0.9861 - val_loss: 0.0767 - val_accuracy: 0.9803\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0448 - accuracy: 0.9884 - val_loss: 0.0691 - val_accuracy: 0.9822\n",
      "Test loss: 0.060047 accuracy: 0.985100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Flatten, Conv2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# 3x3 kernel with stride 2, 32 output channels.\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), input_shape=input_shape, activation=\"relu\"))\n",
    "\n",
    "# 3x3 kernel with stride 2, 64 output channels.\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), activation=\"relu\"))\n",
    "\n",
    "# Flatten the output to be fed to a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer with L2 regularization\n",
    "model.add(Dense(num_classes, activation='softmax', activity_regularizer=l2(0.01)))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss: %f accuracy: %f\" % (score[0], score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Dropouts rather than L2 penalty for regulatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 13, 13, 32)        320       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 6, 6, 64)          18496     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                23050     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41866 (163.54 KB)\n",
      "Trainable params: 41866 (163.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 13s 8ms/step - loss: 0.2987 - accuracy: 0.9091 - val_loss: 0.1091 - val_accuracy: 0.9680\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 13s 8ms/step - loss: 0.1251 - accuracy: 0.9620 - val_loss: 0.0756 - val_accuracy: 0.9785\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0965 - accuracy: 0.9705 - val_loss: 0.0678 - val_accuracy: 0.9814\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0829 - accuracy: 0.9745 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0714 - accuracy: 0.9774 - val_loss: 0.0545 - val_accuracy: 0.9837\n",
      "Test loss: 0.043123 accuracy: 0.986100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# 3x3 kernel with stride 2, 32 output channels.\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), input_shape=input_shape, activation=\"relu\"))\n",
    "\n",
    "# 3x3 kernel with stride 2, 64 output channels.\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), activation=\"relu\"))\n",
    "\n",
    "#Can also use dropouts rather than L2 penalty for regularisation â†’ using dropouts is popular in ConvNets\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flatten the output to be fed to a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer with L2 regularization\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss: %f accuracy: %f\" % (score[0], score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use max-pool to downsample, stride=kernel size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                31370     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50186 (196.04 KB)\n",
      "Trainable params: 50186 (196.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 25s 16ms/step - loss: 0.1787 - accuracy: 0.9494 - val_loss: 0.0755 - val_accuracy: 0.9787\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0629 - accuracy: 0.9835 - val_loss: 0.0584 - val_accuracy: 0.9858\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0483 - accuracy: 0.9882 - val_loss: 0.0498 - val_accuracy: 0.9888\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0394 - accuracy: 0.9903 - val_loss: 0.0476 - val_accuracy: 0.9886\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0323 - accuracy: 0.9929 - val_loss: 0.0484 - val_accuracy: 0.9895\n",
      "Test loss: 0.042377 accuracy: 0.990400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# 3x3 kernel with stride 2, 32 output channels.\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape, padding = \"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3x3 kernel with stride 2, 64 output channels.\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding = \"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output to be fed to a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer with L2 regularization\n",
    "model.add(Dense(num_classes, activation='softmax', activity_regularizer=l2(0.01)))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss: %f accuracy: %f\" % (score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item-Based Movie Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  item  rating  timestamp\n",
      "0   196   242     3.0  881250949\n",
      "1   186   302     3.0  891717742\n",
      "2    22   377     1.0  878887116\n",
      "3   244    51     2.0  880606923\n",
      "4   166   346     1.0  886397596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "found multiple BLAS layers, using first\n",
      "BLAS using multiple threads - can cause oversubscription\n",
      "found 1 potential runtime problems - see https://boi.st/lkpy-perf\n",
      "c:\\Users\\M2-Winterfell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lenskit\\algorithms\\item_knn.py:120: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1munsafe cast from uint64 to int64. Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\n",
      "  b = blocks[bi]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  item  rating  prediction Algorithm\n",
      "0     8   358     2.0    2.815632        II\n",
      "1     8    79     4.0    4.512595        II\n",
      "2     8   403     4.0    3.853674        II\n",
      "3     8    89     4.0    4.481309        II\n",
      "4     8   686     3.0    3.880984        II\n",
      "RMSE II: 0.911671216170855\n"
     ]
    }
   ],
   "source": [
    "from lenskit.datasets import ML100K\n",
    "from lenskit import batch, topn, util\n",
    "from lenskit import crossfold as xf\n",
    "from lenskit.algorithms import Recommender, als, item_knn as knn\n",
    "from lenskit.metrics.predict import rmse\n",
    "import pandas as pd\n",
    "\n",
    "algo_ii = knn.ItemItem(20)\n",
    "\n",
    "def eval(aname, algo, train, test, all_preds):\n",
    "    fittable = util.clone(algo)\n",
    "    fittable = Recommender.adapt(fittable)\n",
    "    fittable.fit(train)\n",
    "    # predict ratings\n",
    "    preds = batch.predict(fittable, test)\n",
    "    preds['Algorithm'] = aname\n",
    "    all_preds.append(preds)\n",
    "\n",
    "def main():\n",
    "    all_preds = []; test_data = []\n",
    "    for train, test in xf.partition_users(ratings[['user', 'item', 'rating']], 5, xf.SampleFrac(0.2)):\n",
    "        test_data.append(test)\n",
    "        eval('II', algo_ii, train, test, all_preds)\n",
    "    preds = pd.concat(all_preds, ignore_index=True)\n",
    "    preds_ii = preds[preds['Algorithm'].str.match('II')]\n",
    "    print(preds_ii.head())\n",
    "    test_data = pd.concat(test_data, ignore_index=True)\n",
    "    print('RMSE II:', rmse(preds_ii['prediction'],preds_ii['rating']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ml100k = ML100K('ml-100k')\n",
    "    ratings = ml100k.ratings; print(ratings.head())\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  item  rating  timestamp\n",
      "0   196   242     3.0  881250949\n",
      "1   186   302     3.0  891717742\n",
      "2    22   377     1.0  878887116\n",
      "3   244    51     2.0  880606923\n",
      "4   166   346     1.0  886397596\n",
      "RMSE BIAS: 0.9445484540190382\n",
      "RMSE II: 0.9048101810159139\n",
      "RMSE MF: 0.9152814723160632\n"
     ]
    }
   ],
   "source": [
    "from lenskit.algorithms.basic import Bias, Popular, TopN\n",
    "\n",
    "algo_pop = Bias(); algo_ii = knn.ItemItem(20)\n",
    "algo_als5 = als.BiasedMF(5)\n",
    "\n",
    "def eval(aname, algo, train, test, all_preds):\n",
    "    fittable = util.clone(algo)\n",
    "    fittable = Recommender.adapt(fittable)\n",
    "    fittable.fit(train)\n",
    "    # predict ratings\n",
    "    preds = batch.predict(fittable, test)\n",
    "    preds['Algorithm'] = aname\n",
    "    all_preds.append(preds)\n",
    "\n",
    "def main():\n",
    "    all_preds = []; test_data = []\n",
    "    for train, test in xf.partition_users(ratings[['user', 'item', 'rating']], 5, xf.SampleFrac(0.2)):\n",
    "        test_data.append(test)\n",
    "        eval('BIAS', algo_pop, train, test, all_preds)\n",
    "        eval('II', algo_ii, train, test, all_preds)\n",
    "        eval('MF', algo_als5, train, test, all_preds)\n",
    "    preds = pd.concat(all_preds, ignore_index=True)\n",
    "    preds_ii = preds[preds['Algorithm'].str.match('II')]\n",
    "    preds_bias = preds[preds['Algorithm'].str.match('BIAS')]\n",
    "    preds_nf= preds[preds['Algorithm'].str.match('MF')]\n",
    "    test_data = pd.concat(test_data, ignore_index=True)\n",
    "    print('RMSE BIAS:', rmse(preds_bias['prediction'],preds_bias['rating']))\n",
    "    print('RMSE II:', rmse(preds_ii['prediction'],preds_ii['rating']))\n",
    "    print('RMSE MF:', rmse(preds_nf['prediction'],preds_nf['rating']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ml100k = ML100K('ml-100k')\n",
    "    ratings = ml100k.ratings; print(ratings.head())\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
